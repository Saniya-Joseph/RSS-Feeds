{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\saniya\\anaconda3\\lib\\site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\saniya\\anaconda3\\lib\\site-packages (from feedparser) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSS Feed Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Some on-air claims about Dominion Voting Systems were false, Fox News acknowledges in statement after deal is announced',\n",
       "  'content': '',\n",
       "  'pub_date': 'Wed, 19 Apr 2023 12:44:51 GMT',\n",
       "  'source_url': 'https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/index.html'},\n",
       " {'title': 'Dominion still has pending lawsuits against election deniers such as Rudy Giuliani and Sidney Powell',\n",
       "  'content': '',\n",
       "  'pub_date': '',\n",
       "  'source_url': 'https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/h_8d51e3ae2714edaa0dace837305d03b8'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "RSS_FEEDS = [\n",
    "    \"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
    "    \"http://qz.com/feed\",\n",
    "    \"http://feeds.foxnews.com/foxnews/politics\",\n",
    "    \"http://feeds.reuters.com/reuters/businessNews\",\n",
    "    \"http://feeds.feedburner.com/NewshourWorld\",\n",
    "    \"https://feeds.bbci.co.uk/news/world/asia/india/rss.xml\"\n",
    "]\n",
    "\n",
    "def parse_feeds():\n",
    "    articles = []\n",
    "    for feed_url in RSS_FEEDS:\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        for entry in feed.entries:\n",
    "            article = {\n",
    "                \"title\": entry.get(\"title\", \"\"),\n",
    "                \"content\": entry.get(\"summary\", \"\"),\n",
    "                \"pub_date\": entry.get(\"published\", \"\"),\n",
    "                \"source_url\": entry.get(\"link\", \"\")\n",
    "            }\n",
    "            articles.append(article)\n",
    "    return articles\n",
    "\n",
    "# Test parsing\n",
    "parsed_articles = parse_feeds()\n",
    "parsed_articles[:2]  # Show first 2 articles for a quick check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Setup Using SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANIYA\\AppData\\Local\\Temp\\ipykernel_10636\\2150875916.py:11: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, String, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import datetime\n",
    "import feedparser\n",
    "\n",
    "# Use SQLite for simplicity if you don't have PostgreSQL\n",
    "DATABASE_URL = 'sqlite:///news_articles.db'  # For local development\n",
    "# DATABASE_URL = \"postgresql://user:password@localhost/rss_feed\"  # Use this if PostgreSQL is available\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class NewsArticle(Base):\n",
    "    __tablename__ = 'news_articles'\n",
    "    title = Column(String, primary_key=True)\n",
    "    content = Column(String)\n",
    "    pub_date = Column(DateTime)\n",
    "    source_url = Column(String)\n",
    "    category = Column(String)\n",
    "\n",
    "# Connect and create table\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# Function to convert pub_date string to datetime object\n",
    "def convert_pub_date(pub_date_str):\n",
    "    try:\n",
    "        # Convert pub_date string to datetime object\n",
    "        return datetime.strptime(pub_date_str, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    except ValueError:\n",
    "        return None  # Return None if the date format is invalid or missing\n",
    "\n",
    "# Function to store articles\n",
    "def store_articles(articles):\n",
    "    session = Session()\n",
    "    for article in articles:\n",
    "        # Convert the pub_date string to a datetime object\n",
    "        article[\"pub_date\"] = convert_pub_date(article[\"pub_date\"])\n",
    "        \n",
    "        # Check if the article already exists in the database\n",
    "        if not session.query(NewsArticle).filter_by(title=article[\"title\"]).first():\n",
    "            new_article = NewsArticle(**article)\n",
    "            session.add(new_article)\n",
    "    session.commit()\n",
    "    session.close()\n",
    "\n",
    "# Example parsed articles from your feedparser logic (for testing)\n",
    "parsed_articles = [\n",
    "    {\n",
    "        \"title\": \"Example News Title 1\",\n",
    "        \"content\": \"This is the content of the news article.\",\n",
    "        \"pub_date\": \"Wed, 19 Apr 2023 12:44:51 GMT\",\n",
    "        \"source_url\": \"https://example.com/news1\",\n",
    "        \"category\": \"Positive/Uplifting\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Example News Title 2\",\n",
    "        \"content\": \"This is another news article.\",\n",
    "        \"pub_date\": \"Fri, 21 Apr 2023 10:30:00 GMT\",\n",
    "        \"source_url\": \"https://example.com/news2\",\n",
    "        \"category\": \"Natural Disasters\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store parsed articles into the database\n",
    "store_articles(parsed_articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANIYA\\AppData\\Local\\Temp\\ipykernel_10636\\3853352441.py:10: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from sqlalchemy import create_engine, Column, String, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import datetime\n",
    "import feedparser\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = 'sqlite:///news_articles.db'  # For local development\n",
    "Base = declarative_base()\n",
    "\n",
    "class NewsArticle(Base):\n",
    "    __tablename__ = 'news_articles'\n",
    "    title = Column(String, primary_key=True)\n",
    "    content = Column(String)\n",
    "    pub_date = Column(DateTime)\n",
    "    source_url = Column(String)\n",
    "    category = Column(String)\n",
    "\n",
    "# Connect and create table\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# Function to convert pub_date string to datetime object\n",
    "def convert_pub_date(pub_date_str):\n",
    "    try:\n",
    "        return datetime.strptime(pub_date_str, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Function to store articles\n",
    "def store_articles(articles):\n",
    "    session = Session()\n",
    "    for article in articles:\n",
    "        article[\"pub_date\"] = convert_pub_date(article[\"pub_date\"])\n",
    "        if not session.query(NewsArticle).filter_by(title=article[\"title\"]).first():\n",
    "            new_article = NewsArticle(**article)\n",
    "            session.add(new_article)\n",
    "    session.commit()\n",
    "    session.close()\n",
    "\n",
    "# Placeholder for the classify_article function\n",
    "def classify_article(content):\n",
    "    # Replace this logic with your actual classification logic\n",
    "    if \"positive\" in content.lower():\n",
    "        return \"Positive/Uplifting\"\n",
    "    elif \"disaster\" in content.lower():\n",
    "        return \"Natural Disasters\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "# Function to classify articles in batches\n",
    "def classify_articles(articles):\n",
    "    for article in articles:\n",
    "        article[\"category\"] = classify_article(article[\"content\"])\n",
    "    return articles\n",
    "\n",
    "def process_articles(articles, batch_size=10):\n",
    "    for i in range(0, len(articles), batch_size):\n",
    "        batch = articles[i:i + batch_size]\n",
    "        classified_articles = classify_articles(batch)\n",
    "        store_articles(classified_articles)\n",
    "\n",
    "# Example parsed articles (you should replace this with actual parsed data)\n",
    "parsed_articles = [\n",
    "    {\n",
    "        \"title\": \"Example News Title 1\",\n",
    "        \"content\": \"This is the content of the news article mentioning a positive event.\",\n",
    "        \"pub_date\": \"Wed, 19 Apr 2023 12:44:51 GMT\",\n",
    "        \"source_url\": \"https://example.com/news1\",\n",
    "        \"category\": \"Positive/Uplifting\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Example News Title 2\",\n",
    "        \"content\": \"This article is about a natural disaster.\",\n",
    "        \"pub_date\": \"Fri, 21 Apr 2023 10:30:00 GMT\",\n",
    "        \"source_url\": \"https://example.com/news2\",\n",
    "        \"category\": \"Natural Disasters\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call this function to process and categorize all articles\n",
    "process_articles(parsed_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\SANIYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Example News Title 1',\n",
       " 'content': 'This is the content of the news article mentioning a positive event.',\n",
       " 'pub_date': datetime.datetime(2023, 4, 19, 12, 44, 51),\n",
       " 'source_url': 'https://example.com/news1',\n",
       " 'category': 'Positive/Uplifting'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_article(content):\n",
    "    sentiment_score = sid.polarity_scores(content)[\"compound\"]\n",
    "    if sentiment_score > 0.3:\n",
    "        return \"Positive/Uplifting\"\n",
    "    elif \"terrorism\" in content.lower() or \"protest\" in content.lower():\n",
    "        return \"Terrorism / protest / political unrest / riot\"\n",
    "    elif \"earthquake\" in content.lower() or \"flood\" in content.lower():\n",
    "        return \"Natural Disasters\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "# Test classification on the first article\n",
    "sample_article = parsed_articles[0]\n",
    "sample_article[\"category\"] = classify_article(sample_article[\"content\"])\n",
    "sample_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def export_to_csv():\n",
    "    session = Session()\n",
    "    articles = session.query(NewsArticle).all()\n",
    "    df = pd.DataFrame([{\n",
    "        \"title\": a.title,\n",
    "        \"content\": a.content,\n",
    "        \"pub_date\": a.pub_date,\n",
    "        \"source_url\": a.source_url,\n",
    "        \"category\": a.category\n",
    "    } for a in articles])\n",
    "    df.to_csv('news_articles.csv', index=False)\n",
    "\n",
    "# Export to CSV\n",
    "export_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def log_error(e, article_title=\"\"):\n",
    "    logging.error(f\"Error processing article '{article_title}': {e}\")\n",
    "\n",
    "# Example usage: log_error(Exception(\"Sample error\"), \"Sample article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 21:37:25,659 - ERROR - Error processing article 'Example News Title 1': strptime() argument 1 must be str, not datetime.datetime\n",
      "2024-10-07 21:37:25,662 - ERROR - Error processing article 'Example News Title 2': strptime() argument 1 must be str, not datetime.datetime\n"
     ]
    }
   ],
   "source": [
    "def process_article_with_error_handling(article):\n",
    "    try:\n",
    "        article[\"category\"] = classify_article(article[\"content\"])\n",
    "        store_articles([article])\n",
    "    except Exception as e:\n",
    "        log_error(e, article[\"title\"])\n",
    "\n",
    "def process_articles_with_error_handling(articles):\n",
    "    for article in articles:\n",
    "        process_article_with_error_handling(article)\n",
    "\n",
    "# Process all articles with error handling\n",
    "process_articles_with_error_handling(parsed_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
